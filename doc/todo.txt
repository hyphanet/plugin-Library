= TODO =

== Documentation of Quirks and Stuff That Future Programmers Must Know ==

- PRIORITY document TaskComplete handling...
- PRIORITY document SkeletonBTreeMap reliance on isLeaf() and nodeSize() being
  called in the correct places.

== Freenet serialisation ==

- PRIORITY make getIndex() unblocking
  - use LiveArchiver for index root serialiser
  - use SnoopMetadata
- OPTIMISE should have a better format than having BinInfo point to a top-level CHK
  - use MHK or have metadata directly in the parent node (latter reduces node fan-out to 400-500 max)

== Resource usage ==

- Make library use a ThreadPoolExecutor that handles execution of Requests
- Memory handling; discard index data after we're done with it.

== Index semantics ==

- URGENT start doing a WriteableIndex and make the ttab protected again.
- PRIORITY adapt SkeletonBTreeMap.inflate() to work for a submap range too. see
  that class for more details.
- TODO index: "suggested tokens" - suggest what other people should tag this
  index as; they are free to ignore this suggestion (very low priority for now)

== Task handling ==

- TODO maybe have deflate/inflate throw a different checked exception from
  TaskAbortException, forcing them to handle the TaskAbortException thrown by
  pull/push?

== TermEntry semantics ==

- PRIORITY make TokenEntries immutable, and make Yaml stuff to create/uncreate these...
- TODO there should be only one entry per URI per Token (enforce this in the code at some point)

== Request/Progress reporting ==

- TODO probably should synchronize SimpleProgress, and BaseCompositeProgress...
  - although maybe unnecessary since these should all be single-writer multi-reader
    (and the reads are not critical)?
  - ProgressParts.getSubParts / ProgressParts.getParts: iterating might throw ConcurrentModificationException!

== Packaging structure ==

- TODO do we want to merge the library/ and index/ subpackages? or shuffle things around?
- TODO get rid of unnecessary Prefix* files
- TODO restructure the Serialiser.* stuff
- TODO Skeleton/SkeletonMap is does what we need for now, but might able to be generalised a bit

== Misc ==

- TODO use markdown-doclet instead of bliki-doclet
  - not yet - MarkdownJ has bugs relating to parsing of uppercase HTML tags,
    which Std Java Doclet generates, unfortunately :/
- TODO make build.xml output test results better... atm either it has two modes:
  - print test stdout output out in real time, but not stderr
  - print stdout and stderr in a single go after the test is over :/




>>> 7f0a7773dc3d60759145e10b750e706c584d8a7c
>>>
>>> URIKey: - I believe we agreed to index by the concatenation of the
>>> routing key and the uri hash? is that the current plan? you could index
>>> by the uri (toString(false, true) -> ascii), but that wouldn't be fixed
>>> length. i dunno how much locality of reference matters within a given
>>> routing key, if it does you need to either index by the actual uri or
>>> use a locality-preserving hash such as tea hash.
>> i'll need to rethink this at some point. the problem with just
>> concatenation is that you'll end up with a tree that's (length of the
>> first part) deep, with nodes in between that only have 1 children. so atm
>> i've gone with only routingKey (probably will soon change this to
>> routingKey + docName, as SSK uses). it will index small-to-medium sites
>> fine, but will be bad for extremely large (thousands of pages) sites.
>> maybe i could use skip-lists instead of a tree within a tree
>> (http://en.wikipedia.org/wiki/Skip_list).
>
> Maybe, I'm not sure how you'd lazily load a skiplist? I guess you can
> segment each level.
>
> Alternatively, what about allowing a child with a prefix longer than [ my
> prefix ] + 1, provided that all keys in that bucket (with that next-element)
> fit within it? And if and when one is added, create an intermediate subtree?
> Of course this means changing the parent pointer when you move the tree
> downwards, but is that such a big deal? You could have a protected or
> package-local accessor...

it would be easier to just grab a B-tree implementation and modify it to suit
our purposes, lol (and we wouldn't have to hash them into Tokens)... actually i
might do just that instead of coding a custom algorithm. atm though i'm going
to carry on other parts of the project, i can mess about with fine-tuning the
data structure more after the rest is working.



== Schedule ==

 - implement freenet inflate/deflate 1-2 days
 - tidying up index stuff 2-3 days

 - build interdex skeleton 2-3 days
 - implement interdex algorithm 4-5 days

--- gsoc end ---

 - implement filters 2 days
 - implement WriteableIndex and commit algorithms 4-6 days
 - implement crawler...



